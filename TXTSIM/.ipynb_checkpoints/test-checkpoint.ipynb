{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASIC TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enchant dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['produced', 'producer']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stopwords\n",
    "from nltk.corpus import stopwords \n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "text = \"i am produced by a producer\"\n",
    "text = [i for i in text.lower().split() if i not in stop]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stupid', 'dolt', 'stupid_person', 'pudding_head', 'stupe', 'poor_fish', 'pillock', 'pudden-head', 'bore', 'dullard'}\n"
     ]
    }
   ],
   "source": [
    "#synonyms\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonyms = []\n",
    "for syn in wordnet.synsets(\"dullard\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "print(set(synonyms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['the dumb Jack']\n",
      "Verbs: []\n",
      "Adj: ['dumb']\n"
     ]
    }
   ],
   "source": [
    "#POS tagging N/V\n",
    "#import spacy\n",
    "#nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "text = (\"the dumb Jack\")\n",
    "doc = nlp(text)\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "print(\"Adj:\", [token.lemma_ for token in doc if token.pos_ == \"ADJ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Active / Passive\n",
    "from modules.functions import is_active\n",
    "\n",
    "print(is_active(\"He is cutting wood with the saw\"))\n",
    "print(is_active(\"the wood is being cut by him with a saw\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the 3 programs are being run by Jake's amazing computer wonderfully\""
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPARE AND REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "from nltk.corpus import wordnet\n",
    "from num2words import num2words\n",
    "from modules.functions import is_active\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets sentence returns obj with POS tagged\n",
    "def tagSentence(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    obj = {}\n",
    "    obj[\"NOUN\"] = [token.lemma_.lower() for token in doc if token.pos_ == \"NOUN\"]\n",
    "    obj[\"PROPN\"] = [token.lemma_.lower() for token in doc if token.pos_ == \"PROPN\"]\n",
    "    obj[\"PRON\"] = [token.lemma_.lower() for token in doc if token.pos_ == \"PRON\"]\n",
    "    obj[\"VERB\"] = [token.lemma_.lower() for token in doc if token.pos_ == \"VERB\" and token.lemma_.lower() != \"be\"]\n",
    "    obj[\"ADJ\"] = [token.lemma_.lower() for token in doc if token.pos_ == \"ADJ\"]\n",
    "    obj[\"ADV\"] = [token.lemma_.lower() for token in doc if token.pos_ == \"ADV\"]\n",
    "    obj[\"NUM\"] = [token.lemma_.lower() for token in doc if token.pos_ == \"NUM\"]\n",
    "    obj[\"SYM\"] = [token.lemma_.lower() for token in doc if token.pos_ == \"SYM\"]\n",
    "    obj[\"X\"] = [token.lemma_.lower() for token in doc if token.pos_ == \"X\"]\n",
    "    return obj\n",
    "\n",
    "#Only for sent2\n",
    "def reTagSentence(sentence2):\n",
    "    obj = {}\n",
    "    doc = nlp(sentence2)\n",
    "    obj[\"NOUN\"] = [token.lemma_.lower() for token in doc if token.pos_ == \"NOUN\"]\n",
    "    obj[\"PROPN\"] = [token.lemma_.lower() for token in doc if token.pos_ == \"PROPN\"]\n",
    "    obj[\"PRON\"] = [token.lemma_.lower() for token in doc if token.pos_ == \"PRON\"]\n",
    "    obj[\"ADJ\"] = [token.lemma_.lower() for token in doc if token.pos_ == \"ADJ\"]\n",
    "    obj[\"VERB\"] = [token.lemma_.lower() for token in doc if token.pos_ == \"VERB\" and token.lemma_.lower() != \"be\"]\n",
    "    return obj\n",
    "\n",
    "#Gets obj with POS tagged, returns obj with POS tagged + synonyms(only for obj2)\n",
    "def findSynonyms(obj):\n",
    "    #NOUN SYNONYMS FOUND IN COMPARE() ITSELF DUE TO NOUN ORDER CHECK\n",
    "    #ADJ SYNONYMS FOUND IN COMPARE() ITSELF DUE TO ADJ ORDER CHECK\n",
    "    #VERB\n",
    "    if(len(obj[\"VERB\"])>0):\n",
    "        tempArr = []\n",
    "        for word in obj[\"VERB\"]:\n",
    "            for syn in wordnet.synsets(word):\n",
    "                for l in syn.lemmas():\n",
    "                    tempArr.append(l.name())\n",
    "        obj[\"VERB\"] = list(set(obj[\"VERB\"]+tempArr))\n",
    "    \n",
    "    #ADV   \n",
    "    if(len(obj[\"ADV\"])>0):\n",
    "        tempArr = []\n",
    "        for word in obj[\"ADV\"]:\n",
    "            for syn in wordnet.synsets(word):\n",
    "                for l in syn.lemmas():\n",
    "                    tempArr.append(l.name())\n",
    "        obj[\"ADV\"] = list(set(obj[\"ADV\"]+tempArr))\n",
    "    #X  \n",
    "    if(len(obj[\"X\"])>0):\n",
    "        tempArr = []\n",
    "        for word in obj[\"X\"]:\n",
    "            for syn in wordnet.synsets(word):\n",
    "                for l in syn.lemmas():\n",
    "                    tempArr.append(l.name())\n",
    "        obj[\"X\"] = list(set(obj[\"X\"]+tempArr))\n",
    "    return obj\n",
    "     \n",
    "#Comparison\n",
    "def compare(obj1,obj2,sentence2):\n",
    "    #original object 2\n",
    "    obj2a = reTagSentence(sentence2)\n",
    "    result = {}\n",
    "    #NOUN, PROPN, PRON, VERB, ADJ, ADV, NUM, SYM, X - CRITICAL\n",
    "    #NOUN\n",
    "    if(len(obj1[\"NOUN\"])>0):\n",
    "        result[\"NOUN\"] = 0\n",
    "        status = 0\n",
    "        for i in range(len(obj1[\"NOUN\"])): \n",
    "            word1 = obj1[\"NOUN\"][i]\n",
    "            for j in range(len(obj2[\"NOUN\"])): \n",
    "                word2 = obj2[\"NOUN\"][j]\n",
    "                tempArr = []\n",
    "                for syn in wordnet.synsets(word2):\n",
    "                    for l in syn.lemmas():\n",
    "                        tempArr.append(l.name())\n",
    "                for tempWord in tempArr:\n",
    "                    if(word1 == tempWord):\n",
    "                        status = 1\n",
    "                        sentence2 = sentence2.replace(word2,word1)\n",
    "                        result[\"NOUN\"] += 1\n",
    "                        break\n",
    "            if(status == 1):\n",
    "                status = 0\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    #PROPN      \n",
    "    if(len(obj1[\"PROPN\"])>0):\n",
    "        result[\"PROPN\"] = 0\n",
    "        for i in range(len(obj1[\"PROPN\"])): \n",
    "            word1 = obj1[\"PROPN\"][i]\n",
    "            for word2 in obj2[\"PROPN\"]:\n",
    "                if(word1 == word2):\n",
    "                    result[\"PROPN\"] += 1\n",
    "    #PRON               \n",
    "    if(len(obj1[\"PRON\"])>0):\n",
    "        result[\"PRON\"] = 0\n",
    "        for word1 in obj1[\"PRON\"]:\n",
    "            for word2 in obj2[\"PRON\"]:\n",
    "                if(word1 == word2):\n",
    "                    result[\"PRON\"] += 1\n",
    "    #VERB\n",
    "    if(len(obj1[\"VERB\"])>0):\n",
    "        result[\"VERB\"] = 0\n",
    "        for word1 in obj1[\"VERB\"]:\n",
    "            for word2 in obj2[\"VERB\"]:\n",
    "                if(word1 == word2):\n",
    "                    result[\"VERB\"] += 1 \n",
    "    #ADJ \n",
    "    if(len(obj1[\"ADJ\"])>0):\n",
    "        result[\"ADJ\"] = 0\n",
    "        status = 0\n",
    "        for i in range(len(obj1[\"ADJ\"])): \n",
    "            word1 = obj1[\"ADJ\"][i]\n",
    "            for j in range(len(obj2[\"ADJ\"])): \n",
    "                word2 = obj2[\"ADJ\"][j]\n",
    "                tempArr = []\n",
    "                for syn in wordnet.synsets(word2):\n",
    "                    for l in syn.lemmas():\n",
    "                        tempArr.append(l.name())\n",
    "                for tempWord in tempArr:\n",
    "                    if(word1 == tempWord):\n",
    "                        status = 1\n",
    "                        sentence2 = sentence2.replace(word2,word1)\n",
    "                        result[\"ADJ\"] += 1\n",
    "                        break\n",
    "\n",
    "    #ADV\n",
    "    if(len(obj1[\"ADV\"])>0):\n",
    "        result[\"ADV\"] = 0\n",
    "        for word1 in obj1[\"ADV\"]:\n",
    "            for word2 in obj2[\"ADV\"]:\n",
    "                if(word1 == word2):\n",
    "                    result[\"ADV\"] += 1 \n",
    "    #NUM     \n",
    "    if(len(obj1[\"NUM\"])>0):\n",
    "        #NUM2WORD\n",
    "        for i in range(len(obj1[\"NUM\"])):\n",
    "            if(obj1[\"NUM\"][i].isdigit()):\n",
    "                obj1[\"NUM\"][i] = num2words(obj1[\"NUM\"][i])\n",
    "        for i in range(len(obj2[\"NUM\"])):\n",
    "            if(obj2[\"NUM\"][i].isdigit()):\n",
    "                obj2[\"NUM\"][i] = num2words(obj2[\"NUM\"][i])\n",
    "        #NORMAL\n",
    "        result[\"NUM\"] = 0\n",
    "        for word1 in obj1[\"NUM\"]:\n",
    "            for word2 in obj2[\"NUM\"]:\n",
    "                if(word1 == word2):\n",
    "                    result[\"NUM\"] += 1\n",
    "    #SYM\n",
    "    if(len(obj1[\"SYM\"])>0):\n",
    "        result[\"SYM\"] = 0\n",
    "        for word1 in obj1[\"SYM\"]:\n",
    "            for word2 in obj2[\"SYM\"]:\n",
    "                if(word1 == word2):\n",
    "                    result[\"SYM\"] += 1\n",
    "    #X-OTHERS\n",
    "    if(len(obj1[\"X\"])>0):\n",
    "        result[\"X\"] = 0\n",
    "        for word1 in obj1[\"X\"]:\n",
    "            for word2 in obj2[\"X\"]:\n",
    "                if(word1 == word2):\n",
    "                    result[\"X\"] += 1\n",
    "    #TUPLE UNPACK AND GET BOTH VALUES\n",
    "    return result,sentence2\n",
    "\n",
    "#Is active\n",
    "def is_active(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    passive_rule = [{'DEP': 'nsubjpass'}, {'DEP': 'aux', 'OP': '*'}, {'DEP': 'auxpass'}, {'TAG': 'VBN'}]\n",
    "    matcher.add('Passive', None, passive_rule)\n",
    "    matches = matcher(doc)\n",
    "    if matches:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NOUN': ['calculator', 'program'], 'PROPN': ['jake'], 'PRON': [], 'VERB': ['execute'], 'ADJ': ['stupid', 'awesome'], 'ADV': ['marvellously'], 'NUM': ['three'], 'SYM': [], 'X': []}\n",
      "{'NOUN': ['program', 'computer'], 'PROPN': ['jake'], 'PRON': [], 'VERB': ['run'], 'ADJ': ['amazing'], 'ADV': ['wonderfully'], 'NUM': ['3'], 'SYM': [], 'X': []}\n"
     ]
    }
   ],
   "source": [
    "sent1 = \"stupid Jake's awesome calculator is executing the three programs marvellously\"\n",
    "sent2 = \"the 3 programs are being run by Jake's amazing computer wonderfully\"\n",
    "object1 = tagSentence(sent1)\n",
    "object2 = tagSentence(sent2)\n",
    "print(object1)\n",
    "print(object2)\n",
    "#OBJECT1 #TEACHER OBJECT NOT TO BE SYNONYMIZED - TO CHECK IF EVERYTHING IS PRESENT\n",
    "object2 = findSynonyms(object2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stupid Jake's awesome calculator is executing the three programs marvellously\n",
      "the 3 programs are being run by Jake's amazing computer wonderfully\n",
      "{'NOUN': 2, 'PROPN': 1, 'VERB': 1, 'ADJ': 1, 'ADV': 1, 'NUM': 1}\n",
      "stupid Jake's awesome calculator is executing the three programs marvellously\n",
      "the 3 programs are being run by Jake's awesome calculator wonderfully\n"
     ]
    }
   ],
   "source": [
    "print(sent1)\n",
    "print(sent2)\n",
    "res,sent2 = compare(object1,object2,sent2)\n",
    "print(res)\n",
    "print(sent1)\n",
    "print(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#end of compare and replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOUN/ADJECTIVE ORDER CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These sentences will be with replaced words for sent2 alone\n",
    "#For NOUN check\n",
    "def NCsplitOnVerb(sentence):\n",
    "    sent = {}\n",
    "    doc = nlp(sentence)\n",
    "    sent[\"TYPE\"] = is_active(sentence)\n",
    "    sent[\"NPV\"] = [token.lemma_.lower() for token in doc if token.pos_==\"NOUN\" \n",
    "     or token.pos_==\"PRON\" \n",
    "     or token.pos_==\"PROPN\"\n",
    "     or token.pos_==\"VERB\" and token.lemma_.lower()!=\"be\"]\n",
    "    sent[\"NPV\"] = ' '.join(sent[\"NPV\"])\n",
    "    sent[\"V\"] = [token.lemma_.lower() for token in doc if token.pos_==\"VERB\" and token.lemma_.lower()!=\"be\"]\n",
    "    sent[\"NP\"]= sent[\"NPV\"].split(sent[\"V\"][0])\n",
    "    sent[\"NP\"] = [word.replace(' ','') for word in sent[\"NP\"]]\n",
    "    sent[\"NP\"] = [''.join(sorted(word)) for word in sent[\"NP\"]]\n",
    "    return sent\n",
    "\n",
    "#Check order of appearance of a noun\n",
    "def checkOrderOfNoun(sent1,sent2):\n",
    "    sent1obj = NCsplitOnVerb(sent1)\n",
    "    sent2obj = NCsplitOnVerb(sent2)\n",
    "    index1 = []\n",
    "    index2 = []\n",
    "    for i in range(len(sent1obj[\"NP\"])):\n",
    "        index1.append(i)\n",
    "        try:\n",
    "            index2.append(sent2obj[\"NP\"].index(sent1obj[\"NP\"][i]))\n",
    "        except:\n",
    "            return False\n",
    "    if(sent1obj[\"TYPE\"]==sent2obj[\"TYPE\"]):\n",
    "        #len(index1) = len(index2)\n",
    "        for i in range(len(index1)):\n",
    "            if(index1[i]!=index2[i]):\n",
    "                return False\n",
    "        return True\n",
    "    elif(sent1obj[\"TYPE\"]!=sent2obj[\"TYPE\"]):\n",
    "        for i in range(len(index1)):\n",
    "            if(index1[i]==index2[i]):\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For ADJ check\n",
    "def ACsplitOnVerb(sentence):\n",
    "    sent = {}\n",
    "    doc = nlp(sentence)\n",
    "    sent[\"TYPE\"] = is_active(sentence)\n",
    "    sent[\"NPVA\"] = [token.lemma_.lower() for token in doc if token.pos_==\"NOUN\" \n",
    "     or token.pos_==\"PRON\" \n",
    "     or token.pos_==\"PROPN\"\n",
    "     or token.pos_==\"ADJ\"\n",
    "     or token.pos_==\"VERB\" and token.lemma_.lower()!=\"be\"]\n",
    "    sent[\"NPVA\"] = ' '.join(sent[\"NPVA\"])\n",
    "    sent[\"V\"] = [token.lemma_.lower() for token in doc if token.pos_==\"VERB\" and token.lemma_.lower()!=\"be\"]\n",
    "    sent[\"NPA\"]= sent[\"NPVA\"].split(sent[\"V\"][0])\n",
    "    sent[\"NPA\"] = [word.replace(' ','') for word in sent[\"NPA\"]]\n",
    "    sent[\"NPA\"] = [''.join(sorted(word)) for word in sent[\"NPA\"]]\n",
    "    return sent\n",
    "\n",
    "#Check order of appearance of an adjective\n",
    "def checkOrderOfAdjective(sent1,sent2):\n",
    "    sent1obj = ACsplitOnVerb(sent1)\n",
    "    sent2obj = ACsplitOnVerb(sent2)\n",
    "    index1 = []\n",
    "    index2 = []\n",
    "    for i in range(len(sent1obj[\"NPA\"])):\n",
    "        index1.append(i)\n",
    "        try:\n",
    "            index2.append(sent2obj[\"NPA\"].index(sent1obj[\"NPA\"][i]))\n",
    "        except:\n",
    "            return False\n",
    "    if(sent1obj[\"TYPE\"]==sent2obj[\"TYPE\"]):\n",
    "        #len(index1) = len(index2)\n",
    "        for i in range(len(index1)):\n",
    "            if(index1[i]!=index2[i]):\n",
    "                return False\n",
    "        return True\n",
    "    elif(sent1obj[\"TYPE\"]!=sent2obj[\"TYPE\"]):\n",
    "        for i in range(len(index1)):\n",
    "            if(index1[i]==index2[i]):\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkOrderOfNoun(sent1,sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkOrderOfNoun(\"Jack shot Bill\",\"Bill was shot by Jack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkOrderOfAdjective(sent1,sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkOrderOfAdjective(\"hot Jack shot Bill\",\"Bill was shot by hot Jack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###END###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
